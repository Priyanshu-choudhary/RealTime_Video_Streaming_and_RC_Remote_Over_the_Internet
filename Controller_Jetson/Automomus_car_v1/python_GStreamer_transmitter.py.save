import cv2
import gi
import numpy as np
import time 
import tensorrt as trt 
import pycuda.autoinit
from Model_unet import TensorRTUnetSegmentor

# --- GStreamer Setup ---
gi.require_version('Gst', '1.0')
from gi.repository import Gst, GLib

# --- Initialization ---
Gst.init(None)

# Your frame dimensions
WIDTH, HEIGHT = 640, 480
FPS = 30
FRAME_RATE_NUMERATOR = 1 

# Global variables for timestamping
CURRENT_PTS = 0
FRAME_DURATION = int(Gst.SECOND / FPS)


# TensorRT Model Configuration (Passed to Segmentor)
ENGINE_FILE_PATH = "unet_mobilenetv2_Marbel.engine"
MODEL_INPUT_H = 384  
MODEL_INPUT_W = 384  

CAM=0

AWS_RTSP_URL = "rtsp://yadiec2.freedynamicdns.net:8554/cam2"

# LAN pipeline  
# PIPELINE_STR = (
#     "appsrc name=mysource is-live=true format=3 caps=video/x-raw,format=BGR,width={width},height={height},framerate={fps_num}/1 ! "
#     "videoconvert ! "
#     "video/x-raw,format=I420 ! "
#     "nvvidconv ! video/x-raw(memory:NVMM), format=NV12 ! "
#     "nvv4l2h265enc bitrate=500000 control-rate=1 preset-level=1 insert-sps-pps=true ! "
#     "h265parse ! rtph265pay pt=96 config-interval=1 ! "
#     "udpsink host=10.11.253.113 port=5000"
# ).format(width=WIDTH, height=HEIGHT, fps_num=FPS)


# AWS MediaMTX pipeline
PIPELINE_STR = (
    "appsrc name=mysource is-live=true format=3 caps=video/x-raw,format=BGR,width={width},height={height},framerate={fps_num}/1 ! "
    "videoconvert ! "
    "video/x-raw,format=I420 ! "
    "nvvidconv ! "
    "video/x-raw(memory:NVMM),format=NV12 ! "
    "nvv4l2h264enc bitrate=200000 control-rate=1 preset-level=1 insert-sps-pps=true maxperf-enable=1 ! "
    "h264parse ! "
    "rtspclientsink location={location} protocols=tcp do-rtsp-keep-alive=true"
).format(width=WIDTH, height=HEIGHT, fps_num=FPS, location=AWS_RTSP_URL)
gst-launch-1.0 v4l2src device=$DEVICE ! \
    image/jpeg, width=1280, height=720, framerate=30/1 ! \
    jpegdec ! \
    videorate ! video/x-raw, framerate=15/1 ! \
    nvvidconv ! 'video/x-raw(memory:NVMM), format=NV12' ! \
    nvv4l2h264enc bitrate=$BITRATE insert-sps-pps=1 maxperf-enable=1 \
    preset-level=1 control-rate=1 iframeinterval=30 ! \
    h264parse ! \
    queue max-size-buffers=1 leaky=downstream ! \
    rtspclientsink location=$RTSP_URL protocols=tcp do-rtsp-keep-alive=true



pipeline = Gst.parse_launch(PIPELINE_STR)
appsrc = pipeline.get_by_name('mysource')

# --- Helper Function to Push Frames ---
FRAME_DURATION = int(1e9 / FPS)
def push_frame(frame):
    global CURRENT_PTS
    
    frame = np.ascontiguousarray(frame)
    data = frame.tobytes()
    buffer = Gst.Buffer.new_wrapped(data)
    
    # Timing is critical for RTSP stability
    buffer.duration = FRAME_DURATION
    buffer.pts = CURRENT_PTS
    buffer.dts = CURRENT_PTS 
    
    # Increment by duration in nanoseconds
    CURRENT_PTS += FRAME_DURATION
    
    appsrc.emit('push-buffer', buffer)
    return Gst.FlowReturn.OK

# --- Main Execution ---
def main():
    
    # 1. Initialize Segmentor - Pass all necessary configuration constants
    try:
        segmentor = TensorRTUnetSegmentor(
            ENGINE_FILE_PATH, 
            MODEL_INPUT_H, 
            MODEL_INPUT_W, 
            WIDTH, 
            HEIGHT
        )
    except Exception as e:
        print(f"FATAL ERROR: Could not initialize TensorRT Segmentor. Ensure '{ENGINE_FILE_PATH}' exists and pycuda/tensorrt are installed.")
        print(f"Details: {e}")
        return

    # 2. Start GStreamer Pipeline
    pipeline.set_state(Gst.State.PLAYING)
    print("GStreamer pipeline is running. Pushing frames...")

    # 3. Open Camera
    cap = cv2.VideoCapture(f'/dev/video{CAM}', cv2.CAP_V4L2)
    
    if not cap.isOpened():
        print("FATAL ERROR: Camera device is not opening. Check permissions/device name.")
        exit(1)
        
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, WIDTH)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, HEIGHT)
    cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')) 
    
    # Wait for camera to stabilize (Added back in)
    print("Camera initialized. Waiting 0.3 seconds for hardware to stabilize...")
    time.sleep(0.3)
    
    try:
        # Main loop to grab, process, and stream
        while True:
            ret, frame = cap.read()
            
            if not ret:
                print("ERROR: Failed to read frame from camera. Exiting loop.")
                break
                
            # --- 4. Segmentation Inference ---
            mask_np = segmentor.infer(frame)
           
           
            
            # Create a 3-channel colored mask (e.g., magenta)
            color_mask = np.zeros_like(frame, dtype=np.uint8)
            color_mask[mask_np > 0] = [255, 255, 255] # white for the segmented area (BGR)
            
            # --- 5. Overlay Mask onto Frame ---
            alpha = 0.5
            output_frame = cv2.addWeighted(frame, 1.0, color_mask, alpha, 0)
            
            # --- 6. Optional: Draw a line on the final output (Your original drawing) ---
            vp_x = WIDTH // 2
            vp_y = int(HEIGHT * 0.6)
            left_bottom  = (int(WIDTH * 0.2), HEIGHT - 10)
            right_bottom = (int(WIDTH * 0.8), HEIGHT - 10)
            cv2.line(output_frame, left_bottom, (vp_x, vp_y), (0, 255, 0), 2)
            cv2.line(output_frame, right_bottom, (vp_x, vp_y), (0, 255, 0), 2)
            cv2.line(output_frame, (WIDTH//2, vp_y+50), (WIDTH//2, HEIGHT - 10), (0, 255, 255), 1)
            
            # --- 7 darw path ---
            center_points = []

            # Horizontal lines every 50px ONLY in white region
            for y in range(0, mask_np.shape[0], 50):
                row = mask_np[y, :]
                white_indices = np.where(row == 255)[0]

                if len(white_indices) > 0:
                    x1 = white_indices[0]
                    x2 = white_indices[-1]

                    cv2.line(output_frame, (x1, y), (x2, y), (0, 255, 0), 1)

                    x_center = (x1 + x2) // 2     # integer center
                    y_center = y                  # row number

                    cv2.circle(output_frame, (x_center, y_center), 4, (255, 0, 0), -1)



                    center_points.append((x_center, y_center))

            center  = (MODEL_INPUT_W // 2, MODEL_INPUT_H - 1)
            cv2.line(output_frame,center_points[-1],center,(0, 0, 0), 2)

            # --- 7. Push the processed frame ---
            push_frame(output_frame)
            
            # Exit key check
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    finally:
        # Cleanup
        print("Stopping pipeline.")
        appsrc.emit("end-of-stream")
        pipeline.set_state(Gst.State.NULL)
        cap.release()
        cv2.destroyAllWindows()

if __name__ == '__main__':
    main()
